{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffbd431",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What exactly is a feature? Give an example to illustrate your point ?\n",
    "Ans: Features are the basic building blocks of datasets. The quality of the features in your dataset has a major impact on the quality of the insights you will gain when you use that dataset for machine learning.\n",
    "\n",
    "Additionally, different business problems within the same industry do not necessarily require the same features, which is why it is important to have a strong understanding of the business goals of your data science project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be671eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. What are the various circumstances in which feature construction is required ?\n",
    "Ans: The features in your data will directly influence the predictive models you use and the results you can achieve. Your results are dependent on many inter-dependent properties. You need great features that describe the structures inherent in your data. Better features means flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39971586",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Describe how nominal variables are encoded ?\n",
    "Ans: Nominal data is made of discrete values with no numerical relationship between the different categories — mean and median are meaningless. Animal species is one example. For example, pig is not higher than bird and lower than fish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8333e990",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Describe how numeric features are converted to categorical features ?\n",
    "Ans: Binning is one way to convert contineous numerical feature into categorical features. Binning removes the outliers, imporvess the value spread and minimizes the effect of small observation errors. \n",
    "Types of Unsupervised binnings:\n",
    "1) Equal width Binning\n",
    "2) Equal frequency binning\n",
    "3) k-means binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3260338",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Describe the feature selection wrapper approach. State the advantages and disadvantages of this approach ?\n",
    "Ans: Wrapper methods measure the “usefulness” of features based on the classifier performance. In contrast, the filter methods pick up the intrinsic properties of the features (i.e., the “relevance” of the features) measured via univariate statistics instead of cross-validation performance.\n",
    "\n",
    "The wrapper classification algorithms with joint dimensionality reduction and classification can also be used but these methods have high computation cost, lower discriminative power. Moreover, these methods depend on the efficient selection of classifiers for obtaining high accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdc131c",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. When is a feature considered irrelevant? What can be said to quantify it ?\n",
    "Ans: Features are considered relevant if they are either strongly or weakly relevant, and are considered irrelevant otherwise.\n",
    "\n",
    "Irrelevant features can never contribute to prediction accuracy, by definition. Also to quantify it we need to first check the list of features, There are three types of feature selection:\n",
    "\n",
    "Wrapper methods (forward, backward, and stepwise selection)\n",
    "Filter methods (ANOVA, Pearson correlation, variance thresholding)\n",
    "Embedded methods (Lasso, Ridge, Decision Tree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ddc141",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. When is a function considered redundant? What criteria are used to identify features that could be redundant ?\n",
    "Ans: If two features {X1, X2} are highly correlated, then the two features become redundant features since they have same information in terms of correlation measure. In other words, the correlation measure provides statistical association between any given a pair of features.\n",
    "\n",
    "Minimum redundancy feature selection is an algorithm frequently used in a method to accurately identify characteristics of genes and phenotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eb7756",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. What are the various distance measurements used to determine feature similarity ?\n",
    "Ans: Four of the most commonly used distance measures in machine learning are as follows:\n",
    "\n",
    "Hamming Distance.\n",
    "Euclidean Distance\n",
    "Manhattan Distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aa2772",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. State difference between Euclidean and Manhattan distances ?\n",
    "Ans: Euclidean Distance: Euclidean distance is calculated as the square root of the sum of the squared differences between a new point (x) and an existing point (y).\n",
    "\n",
    "The Euclidean distance or Euclidean metric is the \"ordinary\" (i.e.straight-line) distance between two points in Euclidean space.\n",
    "The Euclidean distance between points p and q is the length of the line segment connecting them.  \n",
    "Manhattan Distance: This is the distance between real vectors using the sum of their absolute difference.\n",
    "\n",
    "It is the sum of the lengths of the projections of the line segment between the points onto the coordinate axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac84535",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. Distinguish between feature transformation and feature selection ?\n",
    "Ans: Feature extraction creates a new, smaller set of features that captures most of the useful information in the data. The main difference between them is Feature selection keeps a subset of the original features while feature extraction creates new ones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
